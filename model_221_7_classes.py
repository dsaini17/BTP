# -*- coding: utf-8 -*-
"""model_221_7_classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18uDMo4IEZSjQbuIk3yZMtq-xupB9Lfgt

# Kaggle API
"""

!pip install kaggle
from google.colab import files
uploaded = files.upload()

"""# Downloading Dataset"""

import os
! mkdir .kaggle
! mv kaggle.json .kaggle/
! kaggle competitions download -c 
challenges-in-representation-learning-facial-expression-recognition-challenge --force
! cp ".kaggle/competitions/challenges-in-representation-learning-facial-expression-recognition-challenge/fer2013.tar.gz" .
! tar -xzf fer2013.tar.gz
os.listdir()

"""# Data Preprocessing"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

dataset = pd.read_csv('fer2013/fer2013.csv')
print(dataset.head())

y = dataset.iloc[:,0:1].values

X = np.zeros(shape=(dataset.shape[0],48,48))

for i in range(dataset.shape[0]):
    a = dataset['pixels'][i].split(' ')
    b = [int(x) for x in a]
    c = np.asarray(b,dtype = 'float32')
    d = c.reshape(48,48)
    X[i] = d
    
X = X.astype('float32')
X = X/255

classes = 7

index1 = 28709 # Cross-Validation SET ( Public Test )
index2 = 32298 # Final Test SET ( Private Test )

X_train = X[0:index1,:]
X_validate = X[index1:index2,:]
X_test = X[index2:,:]
y_train = y[0:index1,:]
y_validate = y[index1:index2,:]
y_test = y[index2:,:]

print(X_train.shape)
print(y_train.shape)

# X

"""# Neural Network"""

import keras
from keras.models import Sequential
from keras.layers import Dense,Dropout,Flatten,Activation,Convolution2D,MaxPooling2D,BatchNormalization
from keras.utils import np_utils

# Data Reshaping
y_train = np_utils.to_categorical(y_train,classes)
y_validate = np_utils.to_categorical(y_validate,classes)
y_test = np_utils.to_categorical(y_test,classes)
X_train = X_train.reshape(X_train.shape[0],48,48,1)
X_validate = X_validate.reshape(X_validate.shape[0],48,48,1)
X_test = X_test.reshape(X_test.shape[0],48,48,1)

print(X_train.shape)
print(y_train.shape)

model = Sequential()
# =============================================================================
# Section 1
# =============================================================================
model.add(Convolution2D(128,(4,4),input_shape=(48,48,1),strides = 1))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(Dropout(0.2))

# =============================================================================
# Section 2
# =============================================================================
model.add(Convolution2D(128,(4,4),strides = 1,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))

# =============================================================================
# Section 3
# =============================================================================
model.add(Convolution2D(128,(4,4),strides = 1))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides = 2))
model.add(Dropout(0.2))

# =============================================================================
# Section 4
# =============================================================================
model.add(Convolution2D(128,(4,4),strides = 1,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))

# =============================================================================
# Section 5
# =============================================================================
model.add(Convolution2D(128,(4,4),strides = 1))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides = 2))
model.add(Dropout(0.2))

# =============================================================================
# Section 6
# =============================================================================
model.add(Convolution2D(128,(4,4),strides = 1,padding='same'))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))

# =============================================================================
# Section 7
# =============================================================================
model.add(Convolution2D(128,(2,2),strides = 1))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides = 2))
model.add(Dropout(0.2))


model.add(Flatten())

# =============================================================================
# Section 8
# =============================================================================
model.add(Dense(1024))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(Dropout(0.2))

# =============================================================================
# Section 9
# =============================================================================
model.add(Dense(1024))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
model.add(Dropout(0.2))

# =============================================================================
# Section 10
# =============================================================================
model.add(Dense(classes, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model.summary()

class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.train_losses = []
        self.val_losses = []

    def on_epoch_end(self, batch, logs={}):
        self.train_losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))

class AccHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.train_acc = []
        self.val_acc = []

    def on_epoch_end(self, batch, logs={}):
        self.train_acc.append(logs.get('acc'))
        self.val_acc.append(logs.get('val_acc'))

history1 = LossHistory()
history2 = AccHistory()
from keras.callbacks import ModelCheckpoint
filepath1="weights.best.acc.221_model.hdf5"
filepath2="weights.best.loss.221_model.hdf5"
checkpoint1 = ModelCheckpoint(filepath1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')
checkpoint2 = ModelCheckpoint(filepath2, monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks_list = [checkpoint1,checkpoint2,history1,history2]

model.fit(X_train, y_train, 
          batch_size=128,shuffle = True, epochs=16, verbose=2, validation_data = (X_validate,y_validate),callbacks=callbacks_list)

loss_path = '_Loss_Curve'
plt.plot(history1.train_losses,color = 'blue')
plt.plot(history1.val_losses,color = 'red')
#plt.plot(history.history['val_acc'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('time')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()
# plt.savefig(model_name+loss_path+'.png')

acc_path = '_Accuracy_Curve'
plt.plot(history2.train_acc,color = 'blue')
plt.plot(history2.val_acc,color = 'red')
#plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('time')
plt.legend(['train', 'validation'], loc='lower right')
plt.show()
# plt.savefig(model_name+acc_path+'.png')

print(history1.train_losses)
print(history1.val_losses)
print(min(history1.val_losses))

print(history2.train_acc)
print(history2.val_acc)
print(max(history2.val_acc))

print(X_validate.shape)
print(y_validate.shape)

# Comparison based on Losses ( Min )
model.load_weights(filepath2)

validation_score = model.evaluate(X_validate, y_validate, verbose=1)
test_score = model.evaluate(X_test, y_test, verbose=1)
print(' Validation SET Accuracy = '+ str(validation_score[1]*100))
print(' Test SET Accuracy = '+ str(test_score[1]*100))

# Comparison based on Accuracy ( Max )
model.load_weights(filepath1)

validation_score = model.evaluate(X_validate, y_validate, verbose=1)
test_score = model.evaluate(X_test, y_test, verbose=1)
print(' Validation SET Accuracy = '+ str(validation_score[1]*100))
print(' Test SET Accuracy = '+ str(test_score[1]*100))

"""# Visualization"""

config = model.get_config()

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install -q pydot
import pydot

file = 'weights.hdf5'
model.save_weights(file)

os.listdir()

from google.colab import files
files.download(filepath1)
files.download(filepath2)

import pydot
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot

SVG(model_to_dot(model).create(prog='dot', format='svg'))

score = model.evaluate(X_train,y_train,verbose = 1)
score

"""# Metrics"""

# (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)

y_prob = model.predict(X_test, batch_size=32, verbose=1)
y_pred = [np.argmax(prob) for prob in y_prob]
y_true = [np.argmax(true) for true in y_test]

!pip install brewer2mpl
import matplotlib
import brewer2mpl
set3 = brewer2mpl.get_map('Set3', 'qualitative', 7).mpl_colors

def plot_subjects(start, end, y_pred, y_true, title=False):
    fig = plt.figure(figsize=(12,12))
    emotion = {0:'Angry',1:'Disgust',2:'Fear', 3:'Happy', 4:'Sad',5:'Surprise',6:'Neutral'}
    for i in range(start, end+1):
        input_img = X[i+index2]
        ax = fig.add_subplot(6,6,i+1)
        ax.imshow(input_img, cmap=matplotlib.cm.gray)
        plt.xticks(np.array([]))
        plt.yticks(np.array([]))
        if y_pred[i] != y_true[i]:
            plt.xlabel(emotion[y_true[i]], color='#53b3cb',fontsize=12)
        else:
            plt.xlabel(emotion[y_true[i]], fontsize=12)
        if title:
            plt.title(emotion[y_pred[i]], color='blue')
        plt.tight_layout()
    plt.show()

def plot_probs(start,end, y_prob):
    fig = plt.figure(figsize=(12,12))
    for i in range(start, end+1):
        input_img = X[i+index2]
        ax = fig.add_subplot(6,6,i+1)
        ax.bar(np.arange(0,7), y_prob[i], color=set3,alpha=0.9)
        ax.set_xticks(np.arange(0.5,7.5,1))
        labels = ['angry', 'disgust', 'fear', 'happy' ,'sad', 'surprise' ,'neutral']
        ax.set_xticklabels(labels, rotation=90 ,fontsize=12)
        ax.set_yticks(np.arange(0.0,1.1,0.5))
        plt.tight_layout()
    plt.show()

def plot_subjects_with_probs(start, end, y_prob):
    iter = (end - start)/6
    for i in np.arange(0,iter):
        plot_subjects(int(i*6),int((i+1)*6-1), y_pred, y_true, title=False)
        plot_probs(int(i*6),int((i+1)*6-1), y_prob)

plot_subjects_with_probs(0, 36, y_prob)

def plot_distribution(y_true, y_pred):
    ind = np.arange(1.5,8,1)  # the x locations for the groups
    width = 0.35   
    fig, ax = plt.subplots()
    true = ax.bar(ind, np.bincount(y_true), width, color=set3, alpha=1.0)
    pred = ax.bar(ind + width, np.bincount(y_pred), width, color=set3, alpha=0.7)
    ax.set_xticks(np.arange(1.5,8,1))
    labels = ['angry', 'disgust', 'fear', 'happy' ,'sad', 'surprise' ,'neutral']
    ax.set_xticklabels(labels, rotation=30, fontsize=14)
    ax.set_xlim([1.25, 8.5])
    ax.set_ylim([0, 1000])
    ax.set_title('True and Predicted Label Count (Private)')
    plt.tight_layout()
    plt.show()
    
plot_distribution(y_true, y_pred)

from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.Blues):
    cm = confusion_matrix(y_true, y_pred)
    fig = plt.figure(figsize=(5,5))
    matplotlib.rcParams.update({'font.size': 16})
    ax  = fig.add_subplot(111)
    matrix = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    fig.colorbar(matrix) 
    for i in range(0,7):
        for j in range(0,7):  
            ax.text(j,i,cm[i,j],va='center', ha='center')
    # ax.set_title('Confusion Matrix')
    labels = ['angry', 'disgust', 'fear', 'happy' ,'sad', 'surprise' ,'neutral']
    ticks = np.arange(len(labels))
    ax.set_xticks(ticks)
    ax.set_xticklabels(labels, rotation=45)
    ax.set_yticks(ticks)
    ax.set_yticklabels(labels)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

plot_confusion_matrix(y_true, y_pred, cmap=plt.cm.YlGnBu)

# Heat Map

labels = ['angry', 'disgust', 'fear', 'happy' ,'sad', 'surprise' ,'neutral']

import seaborn as sn
cm = confusion_matrix(y_true, y_pred)
df_cm = pd.DataFrame(cm,index = [labels[i] for i in range(0,7)],columns = [labels[i] for i in range(0,7)])
sn.set(font_scale = 1)
sn.heatmap(df_cm,annot = True)

labels = ['angry', 'disgust', 'fear', 'happy' ,'sad', 'surprise' ,'neutral']

def class_precision(y_true, y_pred, emotion):
    cm = confusion_matrix(y_true, y_pred)
    i = [i for i, label in enumerate(labels) if label == emotion][0]
    col = [cm[j,i] for j in range(0,7)]
    return float(col[i])/sum(col)

def class_recall(y_true, y_pred, emotion):
    cm = confusion_matrix(y_true, y_pred)
    i = [i for i, label in enumerate(labels) if label == emotion][0]
    row = [cm[i,j] for j in range(0,7)]
    return float(row[i])/sum(row)

def class_accuracy(y_true, y_pred, emotion):
    cm = confusion_matrix(y_true, y_pred)
    i = [i for i, label in enumerate(labels) if label == emotion][0]
    tp = cm[i,i]
    fn = sum([cm[i,j] for j in range(0,7) if j != i])
    fp = sum([cm[j,i] for j in range(0,7) if j != i])
    tn = sum([cm[i,j] for j in range(0,7) for i in range(0,7)]) -(tp+fp+fn)
    return float(tp + tn)/sum([tp, fn, fp, tn])

for emotion in labels:
    print (emotion)
    print ('   acc = {}'.format(class_accuracy(y_true, y_pred, emotion)))
    print ('  prec = {}'.format(class_precision(y_true, y_pred, emotion)))
    print ('recall = {}\n'.format(class_recall(y_true, y_pred, emotion)))

from sklearn.metrics import classification_report
print (classification_report(y_true, y_pred, target_names=labels))

n = len(y_true)
y_fir = []
y_sec = []
y_trd = []
for i in range(n):
    if y_true[i] == np.argsort(y_prob[i])[-1]:
        y_fir.append([i, y_true[i]])
    if y_true[i] == np.argsort(y_prob[i])[-2]:
        y_sec.append([i, y_true[i]])
    if y_true[i] == np.argsort(y_prob[i])[-3]:
        y_trd.append([i, y_true[i]])
print (float(len(y_fir))/n)
print (float(len(y_sec))/n)
print (float(len(y_trd))/n)

misclass = [float(len(y_fir))/n, float(len(y_sec))/n, float(len(y_trd))/n]

plt.hist([pair[1] for pair in y_sec])

from sklearn.metrics import hamming_loss
hamming_loss(y_true, y_pred)

def plot_misclass_distribution(y, tags): 
    fig = plt.figure(figsize=(4,4))
    ax1 = fig.add_subplot(1,1,1)
    ax1.bar(np.arange(1,len(y)+1), np.array(y)*100, color=set3, alpha=0.8)
    ax1.set_xticks(np.arange(1.40,len(y)+1.40,1))
    ax1.set_xticklabels(tags, rotation=0, fontsize=14)
    ax1.set_xlim([0.75, len(y)+1])
    ax1.set_ylim([0,100])
    ax1.grid(True)
    ax1.set_title('')
    plt.tight_layout()
    plt.show()
tags = ['True\n positive','Correct\n on 2st','Correct\n on 3rd']
plot_misclass_distribution(misclass, tags)